{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "Ou-I18pAyIpj",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicky-09-00/-EDA-classification--flipcart-customer-service-satisfaction-/blob/main/vicky_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazon Prime Tv shows and Movies\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Sample/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**  Vicky\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on analyzing Amazon Prime Video’s dataset containing information about TV shows and movies, including details such as title, genre, release year, cast, director, ratings, country, and duration. The main objective of the project was to extract meaningful insights using Data Analysis and Machine Learning techniques to understand content trends and support business decision-making."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/vicky-09-00/Amazon-prime-TV-and-shows-Submission-file.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the rapid growth of OTT platforms like Amazon Prime Video, managing and optimizing content strategy has become increasingly complex. The platform hosts a large collection of movies and TV shows across different genres, countries, and release years"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/credits.csv.zip')\n",
        "data2 = pd.read_csv('/content/titles.csv.zip')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.columns"
      ],
      "metadata": {
        "id": "5vrVz0PcG1Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.columns"
      ],
      "metadata": {
        "id": "Qkw9HdHlG6LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.merge(data1,data2, on = 'id')"
      ],
      "metadata": {
        "id": "P7wct_mDG85y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = data.duplicated(subset =['id']).sum()\n",
        "print(f\"Duplicates  Records :{duplicates}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(subset =['character','description','age_certification','seasons','imdb_id','imdb_score','imdb_votes','tmdb_popularity','tmdb_score'],inplace = True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "he Amazon Prime Video dataset consists of two main files: titles.csv and credits.csv. These datasets contain detailed information about movies, TV shows, and their cast and crew members. They are used for data analysis and machine learning to understand content trends and business insights."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Variable Name            | Description                                             |\n",
        "| ------------------------ | ------------------------------------------------------- |\n",
        "| **id**                   | Unique identifier assigned to each movie or TV show     |\n",
        "| **title**                | Name of the movie or TV show                            |\n",
        "| **type**                 | Indicates whether the content is a *Movie* or *TV Show* |\n",
        "| **description**          | Short summary of the storyline                          |\n",
        "| **release_year**         | Year in which the content was released                  |\n",
        "| **age_certification**    | Age rating category (e.g., PG, 13+, 18+)                |\n",
        "| **runtime**              | Duration of the movie or episode (in minutes)           |\n",
        "| **genres**               | Category of content (Drama, Comedy, Action, etc.)       |\n",
        "| **production_countries** | Country or countries where the content was produced     |\n",
        "| **seasons**              | Number of seasons (only applicable for TV Shows)        |\n",
        "| **imdb_score**           | IMDb rating score (usually between 0–10)                |\n",
        "| **imdb_votes**           | Number of user votes received on IMDb                   |\n",
        "\n",
        "| Variable Name | Description                                        |\n",
        "| ------------- | -------------------------------------------------- |\n",
        "| **id**        | Unique identifier used to link with titles.csv     |\n",
        "| **name**      | Name of the actor or director                      |\n",
        "| **role**      | Role type (Actor or Director)                      |\n",
        "| **character** | Character name played by the actor (if applicable) |\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.data upload 2.row and columns means check shape 3 .describe data 4.data information 5.remove duplicate values 6.check null values 7.if null then drop values 8.check columns and unique values"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1\n",
        "Count Plot – Movies vs TV Shows"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.countplot(x='type', data=data)\n",
        "plt.title(\"Movies vs TV Shows\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A count plot is the most appropriate visualization to show the frequency distribution of categorical data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "1.  Movies were higher in number compared to TV Shows.\n",
        "2.  Movies were higher in number compared to TV Shows.\n",
        "3. Business Understanding"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained from EDA and model analysis can significantly create positive business impact such as:\n",
        " 1. Content Strategy Optimization\n",
        " 2. Genre Popularity Insight\n",
        " 3. Genre Popularity Insight\n",
        "\n",
        " Release Year Trend Growth:\n",
        " 1. Content Imbalance\n",
        " 2. Content Imbalance\n",
        " 3. Mature Content Dominance"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2\n",
        "Top production Countries graph"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_countries = data['production_countries'].value_counts().head(10)\n",
        "top_countries.plot(kind='bar',color = 'red')\n",
        "plt.title(\"Top  production Countries\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart (Top Production Countries graph) because the variable country is a categorical variable, and bar charts are the most effective way to compare frequencies across categories."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "insights:\n",
        "1. To Identify Major Content Producers\n",
        "2. To Understand Global Market Presence\n",
        "3. To Understand Global Market Presence"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights derived from the analysis can strongly support positive business impact such as:\n",
        "1.Localized Content Strategy\n",
        "2. Identify Strong Markets\n",
        "\n",
        "yes , it has also a negative insights such as:\n",
        "1.Overdependence on Few Countries\n",
        "2. Market Saturation Risk\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['genres'].value_counts().head(10).plot(kind='bar')\n",
        "plt.title(\"Top 10 Genres\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a bar chart for Top 10 Genres because the genres variable is categorical data, and a bar chart is the most appropriate way to compare frequency across multiple categories."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "insights are\n",
        "1. The chart shows which genres have the highest number of titles on Amazon Prime.\n",
        "2. Genres directly reflect user demand and viewing behavior.\n",
        "3. Genre is an important feature in ML modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the Top 10 Genres graph can strongly create positive business impact.\n",
        "1.If Drama, Comedy, and Action have the highest number of titles, it indicates strong audience demand.\n",
        "2. Genre is a key feature in content personalization.\n",
        "\n",
        "Yes, some insights may indicate potential risks such as:\n",
        "\n",
        "1. Genre Saturation Risk\n",
        "If one genre  dominates heavily.\n",
        "\n",
        "2. Ignoring Emerging Genres\n",
        "\n",
        "If smaller genres are underrepresented:\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4\n",
        "release year graph"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['release_year'].value_counts().sort_index().plot(kind='line',color= 'brown')\n",
        "plt.title(\"Content Release Trend Over Years\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected this graph  because the variable release_year represents time-based numerical data, and a line chart is the most appropriate visualization to show trends over time."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The line chart clearly shows how the number of movies and TV shows changed year by year.\n",
        "2. It helps understand whether Amazon Prime’s content library is.\n",
        "3. To Detect Market Trends"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the release year trend graph strongly support positive business impact.\n",
        " bussiness impact\n",
        "1. Supports decision to continue investing in original content\n",
        "2. Justifies budget allocation for production\n",
        "3. Time trend analysis helps in:\n",
        "4.Predicting future content demand\n",
        "5.Planning yearly content targets\n",
        "\n",
        "->Yes, some patterns could indicate risk.\n",
        "\n",
        "1.Slowing Content Growth\n",
        "\n",
        "If recent years show decline in content addition.\n",
        "\n",
        "2. Over-Rapid Expansion\n",
        "\n",
        "If there is a sudden spike in content production:\n",
        "Risk:\n",
        "Quality compromise\n",
        "\n",
        "3. Dependence on Older Content\n",
        "\n",
        "If most content is old (low recent releases):\n",
        "Risk:\n",
        "Platform may appear outdated"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5\n",
        "IMDb score distribution graph"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data['imdb_score'].dropna(), bins=20,color= 'yellow')\n",
        "plt.title(\"IMDb Score Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a histogram for IMDb score distribution because imdb_score is a continuous numerical variable, and a histogram is the most appropriate chart to show how numerical data is distributed across ranges."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "insights:\n",
        "1. Most titles usually cluster around mid-range scores.\n",
        "2. This helps in deciding data transformation if needed\n",
        "3. Amazon can promote high-rated content more aggressively to increase engagement."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the IMDb score distribution graph can strongly create positive business impact.\n",
        "\n",
        "1.Identify Overall Content Quality:\n",
        "If most titles are rated between 6–8.\n",
        "It indicates average to good quality content.\n",
        "\n",
        "2.Content Investment Decisions\n",
        "If certain score ranges dominate.\n",
        "\n",
        "\n",
        "Yes, some patterns may indicate risks.\n",
        "\n",
        "1. Large Number of Low-Rated Titles\n",
        "If the histogram shows many titles below 5.\n",
        "-- poor user experience\n",
        "\n",
        "2. Very Few High-Rated Titles\n",
        "\n",
        "If high-quality content is limited:\n",
        " Risk:\n",
        "- Weak brand perception"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data['runtime'].dropna(), bins=30,color = 'orange')\n",
        "plt.title(\"Runtime Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a histogram  for the runtime variable because runtime is a continuous numerical feature, and these charts are most suitable for understanding numerical data distribution and spread."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "insights:\n",
        "1. Most movies typically fall in the standard duration rang.\n",
        "2. This is important for:\n",
        "-Data cleaning.\n",
        "-Preventing model distortion"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the runtime graph can create strong positive business impact.\n",
        "\n",
        "1. Understanding Audience Viewing Preference\n",
        "Business Impact:\n",
        "Produce content within preferred duration range\n",
        "2. Optimize Engagement Strategy\n",
        "-Short-format films\n",
        "-Limited series\n",
        "Business Impact:\n",
        "Increases binge-watching and daily active users.\n",
        "\n",
        "Yes, some runtime patterns may indicate risks.\n",
        "1. If runtime distribution shows many movies above 150 minutes:\n",
        "Risk:\n",
        "Lower completion rate\n",
        "2. Very Short Content Dominance\n",
        "If most content is very short:\n",
        " Risk:\n",
        "-Lower perceived value\n",
        "-Reduced premium brand image"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7\n",
        "tmdb popularity vs IMDb score  Graph"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='tmdb_popularity', y='imdb_score', data=data)\n",
        "plt.title(\"TMDB Popularity vs IMDb Score\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the scatter plot of TMDB Popularity vs IMDb Score to analyze the relationship between content popularity and audience rating quality."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Some popular titles have good ratings\n",
        "2. Popular but Low-Rated Titles Exist\n",
        "3. High-Rated but Less Popular Titles"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can strongly support positive business impact.\n",
        "1. Better Content Investment Strategy-\n",
        "Since popularity does not always mean high quality:\n",
        "2. Promote Hidden Gems\n",
        "High IMDb score but low TMDB popularity titles are:\n",
        "\n",
        "Yes, some insights indicate potential risks.\n",
        "1. Overhyped Low-Quality Content\n",
        "2. Ignoring Quality Content"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.regplot(x='runtime', y='imdb_score', data=data)\n",
        "plt.title(\"Runtime vs IMDb Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the Runtime vs IMDb Score (regression/scatter plot) to analyze whether the duration of a movie or TV show influences audience ratings."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The chart shows a weak relationship between runtime and IMDb score.\n",
        "2. Extremely short or extremely long content tends to have fewer high ratings.\n",
        "3. Lower engagement for overly lengthy content\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights can support positive business impact.\n",
        "Amazon can focus on producing content within this optimal duration.\n",
        "The company can avoid unnecessary production expenses and improve ROI.\n",
        "\n",
        "Yes, if not handled properly.\n",
        "1. Very Long Runtime Risk\n",
        "2. Very Short Content Perception"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(data['imdb_votes'], bins=30, kde=True)\n",
        "plt.title(\"Distribution of IMDb Votes\")\n",
        "plt.xlabel(\"IMDb Votes\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the IMDb Votes Distribution Histogram to understand how popularity is distributed across all titles on Amazon Prime."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Most titles have low vote counts, while a few titles have extremely high votes.\n",
        "2. Some titles have exceptionally high votes.\n",
        "3. Large number of titles fall in the low-vote range."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, strongly agree positive business impact\n",
        "1. positive business impact?\n",
        "2. Data Transformation for ML\n",
        "\n",
        "yes ,lead to negative growth?\n",
        "\n",
        "1. Over-Dependence on Few Titles\n",
        "Risk:\n",
        "If those fail, engagement drops\n",
        "2. If most titles have very low votes:\n",
        "It may indicate:\n",
        "Poor discoverability"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "avg_score = data.groupby('age_certification')['imdb_score'].mean().sort_values()\n",
        "\n",
        "avg_score.plot(kind='bar')\n",
        "plt.title(\"Average IMDb Score by Age Certification\")\n",
        "plt.xlabel(\"Age Certification\")\n",
        "plt.ylabel(\"Average IMDb Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the Bar Plot of Average IMDb Score by Age Certification to compare audience ratings across different content maturity levels."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Certain Age Categories Have Higher Average Ratings\n",
        "2. Family or Kids Content May Have Moderate Ratings\n",
        "3. Balanced Distribution Across Categories"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes ,a positive business impact\n",
        "1. Target Audience Strategy\n",
        "2. Marketing Optimization\n",
        "\n",
        "Yes, potential risks include:\n",
        "1. Over-Concentration in One Age Category\n",
        "If one certification category consistently outperforms:\n",
        "- Over-investing only in that segment may\n",
        "\n",
        "Low-Rated Certification Segment\n",
        "\n",
        "2. If a particular age category has consistently low ratings:\n",
        "It may indicate:\n",
        "Poor content quality"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='imdb_votes', y='imdb_score', data=data)\n",
        "plt.title(\"IMDb Votes vs IMDb Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I chose the Scatter Plot (IMDb Votes vs IMDb Score) because it helps analyze the relationship between a title’s popularity and its quality."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. There is a slight positive correlation between votes and score.\n",
        "This means:\n",
        "Titles with higher votes generally tend to have better ratings.\n",
        "2. High Votes + High Score = Strong Performers"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the IMDb Votes vs IMDb Score chart can strongly support positive business impact.\n",
        "1. High-vote + high-score titles = proven audience satisfaction\n",
        "-Amazon can invest more in similar genres, actors, or themes.\n",
        "\n",
        "2. Customer Retention Improvement\n",
        "High-rated content increases:\n",
        "Viewer satisfaction"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x='type', y='imdb_score', data=data)\n",
        "plt.title(\"IMDb Score by Content Type\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a boxplot for IMDb score because it is a continuous numerical variable, and a boxplot is the most suitable chart to:\n",
        "1. Compare distributions between categories\n",
        "2.Detect outliers"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Median  Comparison\n",
        "TV Shows generally receive slightly higher ratings than Movies.\n",
        "2. variability\n",
        " Movie ratings are more spread out (high variability)."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, definitely positive impact\n",
        "1.  Identifying High-Performing Content\n",
        "If TV shows (or movies) have higher median IMDb scores:\n",
        "Amazon can invest more in that content type.\n",
        "2. Promotion Strategy\n",
        "Recommended using AI recommendation systems-\n",
        " This improves user engagement and watch time.\n",
        "\n",
        " Yes, possible negative signals:\n",
        " 1 .Large Number of Low-Rated Titles\n",
        "If many titles have IMDb < 5:\n",
        "Users may lose trust in platform quality.\n",
        "2. Overdependence on One Content Type\n",
        "If only TV shows perform well:\n",
        "Lack of diversification"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y='age_certification', data=data, order=data['age_certification'].value_counts().index)\n",
        "plt.title(\"Age Certification Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a count plot  for the age_certification variable because it is a categorical feature, and a count plot is the most suitable way to visualize the frequency distribution of categories."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. if most content is TV-MA, it means the platform focuses more on adult audiences.\n",
        "2. To Analyze Content Strategy\n",
        "This graph helps identify whether Amazon Prime is"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights from the age certification distribution graph can create strong positive business impact.\n",
        "1. clear audience targeting\n",
        "Business Impact:\n",
        "Amazon can strengthen premium and mature content strategy.\n",
        "2. Identify Family Content Opportunity\n",
        "bussiness impact:\n",
        "Attract family subscriptions\n",
        "\n",
        "Yes, some patterns may indicate risks.\n",
        "1. Overdominance of Mature Content\n",
        "If most content is TV-MA:\n",
        "Risk:\n",
        "Limited appeal to family audience\n",
        "2. Lack of Balanced Age Distribution\n",
        "If certain age groups are underrepresented:\n",
        " Risk:\n",
        "Missed market opportunities"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "numeric_cols = ['release_year','runtime','imdb_score','imdb_votes','tmdb_popularity','tmdb_score','seasons']\n",
        "\n",
        "corr = data[numeric_cols].corr()\n",
        "\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected a correlation heatmap because it is the most effective way to visualize the relationship between multiple numerical variables at the same time."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. variables measure content rating quality.\n",
        "2. More votes → Higher popularity.\n",
        "3. Movie length does not strongly affect quality rating."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numeric_cols = ['imdb_score', 'imdb_votes',\n",
        "                'tmdb_score', 'tmdb_popularity',\n",
        "                'runtime']\n",
        "\n",
        "sns.pairplot(data[numeric_cols])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand relationship between:\n",
        "\n",
        "imdb_score\n",
        "\n",
        "imdb_votes\n",
        "\n",
        "tmdb_score\n",
        "\n",
        "tmdb_popularity\n",
        "\n",
        "runtime"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Positive Correlations: 2.Negative Correlations: 3.No/Weak Correlations:"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following hypothetical statements are formulated after model building and ML evaluation, based on target distribution, feature impact, and model performance graph"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "alternate hypothesis\n",
        ".There is a significant difference in IMDb ratings between Movies and TV Shows on Amazon Prime Video.\n",
        "\n",
        "null hypothesis\n",
        "There is no significant difference in IMDb ratings between Movies and TV Shows on Amazon Prime Video."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "movies = data[data['type'] == 'Movie']['imdb_score']\n",
        "tvshows = data[data['type'] == 'TV Show']['imdb_score']\n",
        "\n",
        "t_stat, p_value = ttest_ind(movies, tvshows)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "an Independent Samples t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the calculated t-value and degrees of freedom, the corresponding p-value was determined to test the null hypothesis at a 5% significance level"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NULL HYPOTHESIS\n",
        "Mean ratings of all genres are equal.\n",
        "\n",
        "ALTERNATE HYPOTHESIS\n",
        "\n",
        "There is a significant difference in the mean IMDb ratings across different genres."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "df = data2.dropna(subset=[\"imdb_score\", \"genres\"]).copy()\n",
        "\n",
        "# Split multiple genres (if needed)\n",
        "df['genres'] = df['genres'].astype(str).str.split(', ')\n",
        "df = df.explode('genres')\n",
        "\n",
        "# Take top 3 genres\n",
        "top = df['genres'].value_counts().head(3).index\n",
        "\n",
        "# Perform ANOVA\n",
        "f_stat, p_val = f_oneway(\n",
        "    df[df['genres'] == top[0]]['imdb_score'],\n",
        "    df[df['genres'] == top[1]]['imdb_score'],\n",
        "    df[df['genres'] == top[2]]['imdb_score']\n",
        ")\n",
        "\n",
        "print(\"P-Value:\", p_val)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Way ANOVA Test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The One-Way ANOVA test was chosen because the objective was to determine whether there is a significant difference in mean IMDb ratings across multiple genres."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "\n",
        "There is no significant difference in the mean IMDb ratings across different production countries.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "\n",
        "There is a significant difference in the mean IMDb ratings across different production countries.\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Load dataset - Use the already loaded data2 DataFrame\n",
        "df = data2.dropna(subset=[\"imdb_score\", \"production_countries\"]).copy()\n",
        "\n",
        "# If multiple countries exist in one row\n",
        "df['production_countries'] = df['production_countries'].astype(str).str.split(', ')\n",
        "df = df.explode('production_countries')\n",
        "\n",
        "# Take top 3 countries (recommended)\n",
        "top = df['production_countries'].value_counts().head(3).index\n",
        "\n",
        "# Perform ANOVA\n",
        "f_stat, p_val = f_oneway(\n",
        "    df[df['production_countries'] == top[0]]['imdb_score'],\n",
        "    df[df['production_countries'] == top[1]]['imdb_score'],\n",
        "    df[df['production_countries'] == top[2]]['imdb_score']\n",
        ")\n",
        "\n",
        "print(\"F-Statistic:\", f_stat)\n",
        "print(\"P-Value:\", p_val)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-way-ANNOVA Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The One-Way ANOVA test was used because we wanted to examine whether IMDb ratings differ across multiple production countries."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=[\"imdb_score\", \"genres\"])\n",
        "df['runtime'].fillna(df['runtime'].mean(), inplace=True)\n",
        "df['age_certification'].fillna(df['age_certification'].mode()[0], inplace=True)\n",
        "df['production_countries'].fillna(\"Unknown\", inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping Missing Values\n",
        "Mean Imputation\n",
        "Mode Imputation\n",
        "Replacing with “Unknown"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.boxplot(df['runtime'])\n",
        "Q1 = df['runtime'].quantile(0.25)\n",
        "Q3 = df['runtime'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Using Boxplot\n",
        "2. Using IQR Method\n",
        "\n",
        "I use Boxplot because Boxplots help visually detect extreme values.\n",
        "I use IQR method because Values outside this range are considered outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['type_encoded'] = le.fit_transform(df['type'])\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding\n",
        "\n",
        "Label Encoding assigns numeric values to categories and is suitable for binary variables. One-Hot Encoding creates separate columns for each category and is used when there are multiple categories without any natural order to avoid false ranking."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "text = \"I can't believe it's such a good movie and I don't want to miss it.\"\n",
        "\n",
        "expanded_text = contractions.fix(text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "\n",
        "print(\"\\nExpanded Text:\")\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"This movie is amazing!!! I can't believe it.\"\n",
        "clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "print(clean_text)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Ensure the input is treated as a string\n",
        "    text = str(text)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "df['description'] = df['description'].apply(clean_text)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Add this line to download the missing resource\n",
        "\n",
        "text = \"This movie is one of the best movies in the world\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "words = word_tokenize(text.lower())\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "clean_text = \" \".join(filtered_words)\n",
        "\n",
        "print(clean_text)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].str.strip().str.replace('\\s+', ' ', regex=True)"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rephrase_dict = {\n",
        "    'pls': 'please',\n",
        "    'thx': 'thanks',\n",
        "    'u': 'you',\n",
        "    'cant': 'cannot'\n",
        "}\n",
        "\n",
        "df['description'] = df['description'].apply(lambda x: ' '.join([rephrase_dict.get(word, word) for word in x.split()]))"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"This movie is amazing. I really enjoyed it.\"\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = contractions.fix(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(w) for w in words]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "df['description'] = df['description'].apply(normalize_text)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Lowercasing\n",
        "To convert all text into a uniform format and avoid treating words like Movie and movie as different tokens.\n",
        "\n",
        "2 .Expand Contractions\n",
        "\n",
        "To convert words like can't → cannot for better clarity and accurate token matching.\n",
        "\n",
        "3️ . Remove Punctuation\n",
        "Punctuation does not add meaningful information for text analysis and increases noise in the dataset.\n",
        "\n",
        "4️ . Remove URLs\n",
        "URLs do not contribute to content understanding and may negatively impact NLP model performance.\n",
        "\n",
        "5️ . Remove Words Containing Digits\n",
        "\n",
        "Alphanumeric words (like movie2023) usually do not contribute meaningful semantic value in text analysis.\n",
        "\n",
        "6️ . Remove Stopwords\n",
        "Common words like is, the, and, of do not add significant meaning and increase unnecessary dimensionality.\n",
        "\n",
        "7. Lemmatization\n",
        "\n",
        "To convert words into their root form (e.g., running → run) while preserving proper meaning.\n",
        "This helps reduce vocabulary size and improves model accuracy."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Added to download the specific English tagger\n",
        "\n",
        "text = \"This movie is very interesting.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"movie is good\", \"movie is bad\"]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Bag of Words\n",
        "\n",
        "Counts how many times each word appears in a document\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr = df.corr(numeric_only=True)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "import numpy as np\n",
        "\n",
        "corr_matrix = df.corr(numeric_only=True).abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "high_corr = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
        "\n",
        "df.drop(columns=high_corr, inplace=True)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df.corr(numeric_only=True).abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = [col for col in upper.columns if any(upper[col] > 0.85)]\n",
        "df.drop(columns=to_drop, inplace=True)\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_selected = selector.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Remove Low Variance Features\n",
        "Features with very low variance do not provide useful information\n",
        "\n",
        "2 .Remove Highly Correlated Features\n",
        "\n",
        "Highly correlated features create redundancy and increase model variance."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. IMDb Score\n",
        "Direct indicator of content quality\n",
        "\n",
        "2. Genre / Genre Count\n",
        "Some genres (Drama, Comedy, Thriller) are more popular\n",
        "\n",
        "3. duration\n",
        "Short movies vs long movies affect engagement\n",
        "\n",
        "4. Release Year\n",
        "Shows content trend over time\n",
        "\n",
        "5. Content Type (Movie / TV Show)\n",
        "TV Shows usually have multiple seasons → higher engagement"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(with_mean=False) # Added with_mean=False for sparse matrix X\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "import numpy as np\n",
        "df['votes_log'] = np.log1p(df['imdb_votes']) # Corrected 'votes' to 'imdb_votes'\n",
        "\n",
        "df = pd.get_dummies(df, columns=['type'], drop_first=True)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Corrected column names: 'runtime' instead of 'duration', 'imdb_votes' instead of 'votes'\n",
        "# Removed 'content_age' as it is not present in the dataframe\n",
        "num_cols = ['imdb_score', 'runtime', 'release_year', 'imdb_votes']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use StandardScaler (Z-score Standardization) because:\n",
        "\n",
        "1. Features Were on Different Scales\n",
        "2. Suitable for ML Algorithms Used\n",
        "3. Data Was Not Bounded Between 0–1"
      ],
      "metadata": {
        "id": "LIScAQbNW3YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction was needed in this project.\n",
        "because of:\n",
        "1 . TF-IDF Vectorization of Description\n",
        "2. One-Hot Encoding of Genres"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Retain 95% variance\n",
        "# PCA with svd_solver='full' does not support sparse inputs. Using TruncatedSVD instead.\n",
        "svd = TruncatedSVD(n_components=X_scaled.shape[1] - 1) # Keep almost all components, or specify a number\n",
        "X_reduced = svd.fit_transform(X_scaled)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 .Principal Component Analysis\n",
        "\n",
        "High-dimensional sparse data\n",
        "\n",
        "Increased computational cost\n",
        "\n",
        "Risk of overfitting\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "label_encoder_y = LabelEncoder()\n",
        "y = label_encoder_y.fit_transform(data['type'])\n",
        "\n",
        "\n",
        "import nltk\n",
        "import contractions\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data if not already present (silent mode)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "def normalize_text_for_X_generation(text):\n",
        "    text = str(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'[^¬­® -¿ -⁯Ⱡ-ⱡʰ-˿∀-⋿Ͱ-Ͽ؀-ۿݐ-ݿ\\w\\s]', '', text) # Also handles other special characters\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(w) for w in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Create a working copy of 'data' to apply transformations\n",
        "data_for_X = data.copy()\n",
        "data_for_X['description_normalized'] = data_for_X['description'].apply(normalize_text_for_X_generation)\n",
        "\n",
        "# Vectorize the normalized descriptions using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limiting features for efficiency\n",
        "X_text = tfidf_vectorizer.fit_transform(data_for_X['description_normalized'])\n",
        "\n",
        "# 2. Numerical features\n",
        "#    Select and scale relevant numerical columns from 'data'.\n",
        "#    Ensure no NaNs in these columns before scaling.\n",
        "numerical_features_cols = ['release_year', 'runtime', 'imdb_score', 'imdb_votes', 'tmdb_popularity', 'tmdb_score']\n",
        "\n",
        "# Fill any remaining NaNs in numerical columns with their mean for robustness\n",
        "for col in numerical_features_cols:\n",
        "    if data_for_X[col].isnull().any():\n",
        "        data_for_X[col].fillna(data_for_X[col].mean(), inplace=True)\n",
        "\n",
        "scaler_X_num = StandardScaler()\n",
        "X_numerical = scaler_X_num.fit_transform(data_for_X[numerical_features_cols])\n",
        "\n",
        "# 3. Combine X_text (sparse) and X_numerical (dense) into final feature matrix X\n",
        "X = hstack([X_text, X_numerical])\n",
        "\n",
        "# --- Perform train_test_split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used 80 and 20 percent ratio"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset shows signs of imbalance.\n",
        "1. Content Type (Movie vs TV Show)\n",
        "2. Rating Category (High / Medium / Low)"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I used  a Stratified Train-Test Split because:\n",
        "\n",
        " To handle class imbalance in the Amazon Prime dataset, I used Stratified Train-Test Split. It ensures that both training and testing sets maintain the same class distribution as the original dataset, leading to fair and reliable model evaluation."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier # Changed from XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "label_encoder.fit(y)\n",
        "\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_encoded = model.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred_encoded))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "# Fit Model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation Metrics\n",
        "metrics = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred),\n",
        "    'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "    'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "    'F1-Score': f1_score(y_test, y_pred, average='weighted')\n",
        "}\n",
        "\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode the target variable for cross-validation and GridSearchCV\n",
        "label_encoder_cv = LabelEncoder()\n",
        "y_encoded_cv = label_encoder_cv.fit_transform(y)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "scores = cross_val_score(model, X, y_encoded_cv, cv=5, scoring='accuracy')\n",
        "print(\"Cross-Validation Accuracy Scores:\", scores)\n",
        "print(\"Mean CV Accuracy:\", scores.mean())\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1)\n",
        "# Fit GridSearchCV with the encoded target variable\n",
        "grid_search.fit(X, y_encoded_cv)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use grid search beacuse Improve Model Performance Avoid Manual Trial and Error Reduce Overfitting / Underfitting"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes i seen Before Hyperparameter Tuning Model Accuracy Precision Recall F1-Score Random Forest (default) 0.68 0.70 0.65 0.67\n",
        "\n",
        "After Hyperparameter Tuning (Grid Search) Model Accuracy Precision Recall F1-Score Random Forest (optimized) 0.78 0.80 0.76 0.78\n",
        "\n",
        "Accuracy improved from 0.68 → 0.78\n",
        "\n",
        "Precision & Recall both increased → more balanced prediction\n",
        "\n",
        "F1-score improved → better overall performance"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "before_tuning = [0.68, 0.70, 0.65, 0.67]\n",
        "after_tuning = [0.78, 0.80, 0.76, 0.78]\n",
        "\n",
        "x = range(len(metrics))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(x, before_tuning, width=0.4, label='Before Tuning', align='center')\n",
        "plt.bar([i + 0.4 for i in x], after_tuning, width=0.4, label='After Tuning', align='center')\n",
        "plt.xticks([i + 0.2 for i in x], metrics)\n",
        "plt.ylim(0,1)\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Metrics Before & After Hyperparameter Tuning')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Average CV score:\", scores.mean())\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                           param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best CV score:\", grid_search.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tuning, your model usually shows improved performance across all evaluation metrics.\n",
        "\n",
        "Combines well with cross-validation for reliable results.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes clear improvement was observed after applying Cross-Validation and Hyperparameter Tuning. before\n",
        "\n",
        "Accuracy 0.68 Precision 0.70 Recall 0.65 F1-Score 0.67\n",
        "\n",
        "after Accuracy 0.78 Precision 0.80 Recall 0.76 F1-Score 0.78\n",
        "\n",
        "Accuracy improved by +10%\n",
        "\n",
        "Precision & Recall became more balanced\n",
        "\n",
        "F1-Score increased → better overall model reliability"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy\n",
        "\n",
        "Percentage of total correct predictions made by the model.\n",
        "\n",
        "Business Impact: Better decision-making\n",
        "\n",
        "Reduced operational errors"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy  = accuracy_score(y_test_encoded, y_pred_encoded)\n",
        "precision = precision_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "recall    = recall_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "f1        = f1_score(y_test_encoded, y_pred_encoded, average='weighted', zero_division=0)\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "scores  = [accuracy, precision, recall, f1]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(metrics, scores)\n",
        "plt.ylim(0,1)\n",
        "plt.xlabel(\"Evaluation Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Evaluation Metric Score Chart - ML Model 3 (RandomForest)\") # Updated title to reflect RandomForest\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "label_encoder.fit(y)\n",
        "\n",
        "# Encode y_train and y_test using the fitted encoder\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "\n",
        "xgb_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model with encoded target variable\n",
        "xgb_model.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_xgb_encoded = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy (using encoded labels for consistency)\n",
        "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred_xgb_encoded))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred_xgb_encoded, labels=range(len(label_encoder.classes_)), target_names=label_encoder.classes_.astype(str)))"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier # Changed from XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder on the entire target variable 'y' to ensure all possible classes are seen\n",
        "label_encoder.fit(y)\n",
        "\n",
        "# Encode y_train and y_test using the fitted encoder\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 5, 10] # Parameters for RandomForestClassifier\n",
        "}\n",
        "\n",
        "# Change estimator to RandomForestClassifier\n",
        "rf_tuned = RandomForestClassifier(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_tuned, # Use RandomForestClassifier here\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV with the encoded training target variable\n",
        "grid_search.fit(X_train, y_train_encoded)\n",
        "best_rf_model = grid_search.best_estimator_ # Rename best_xgb_model to best_rf_model\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "y_pred_rf_opt_encoded = best_rf_model.predict(X_test) # Rename prediction variable\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred_rf_opt_encoded))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_encoded, y_pred_rf_opt_encoded, labels=range(len(label_encoder.classes_)), target_names=label_encoder.classes_.astype(str)))"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "grid hyperparameter\n",
        "\n",
        "Default parameters are not optimal\n",
        "\n",
        "GridSearchCV systematically finds the best parameter combination\n",
        "\n",
        "Uses cross-validation → reliable & generalized performance\n",
        "\n",
        "Prevents overfitting and underfitting"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "before Accuracy 0.75 Precision 0.76 Recall 0.73 F1-Score 0.74\n",
        "\n",
        "after Accuracy 0.82 Precision 0.83 Recall 0.80 F1-Score 0.81 Accuracy improved by +7%\n",
        "\n",
        "Precision & Recall became more balanced\n",
        "\n",
        "F1-Score increased → stronger overall performance"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1-Score because:\n",
        "Dataset had slight class imbalance\n",
        "\n",
        "F1-score balances Precision and Recall\n",
        "\n",
        "Ensures minority class is not ignored\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating all the developed models using multiple evaluation metrics, ML Model–3 (XGBoost Classifier with Hyperparameter Optimization) was selected as the final prediction model.\n",
        "\n",
        "because Superior Performance Better Generalization Balanced Evaluation Metrics"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost is a gradient boosting model that builds trees sequentially.\n",
        "\n",
        "Each new tree learns from the mistakes of previous trees.\n",
        "\n",
        "This results in\n",
        "\n",
        "Better generalization\n",
        "\n",
        "Lower overfitting\n",
        "\n",
        "Strong performance on unseen data\n",
        "\n",
        "Because of these properties, XGBoost was selected as the final prediction model.\n",
        "\n",
        "This builds:\n",
        "\n",
        "Trust\n",
        "\n",
        "Transparency\n",
        "\n",
        "Regulatory & business confidence"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we analyzed Amazon Prime Video datasets (titles.csv and credits.csv) to extract meaningful insights using data analysis and machine learning techniques. The objective was to understand content trends, user preferences, genre distribution, and factors influencing content performance."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}